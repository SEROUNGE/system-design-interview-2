# 2장. 주변 친구
## 0단계. 들어가며
이번 장에서는 **'주변 친구'라는 모바일 앱 기능을 지원하는 규모 확장성이 용이한 백엔드 시스템을 설계**해보도록 하겠습니다. </br>
'주변 친구' 기능이란, 앱 사용자 가운데 본인의 위치 정보 접근 권한을 허락한 사용자에 한해, 인근의 친구 목록을 보여주는 시스템입니다.

---
## 1단계. 문제 이해 및 설계 범위 확정
질문을 던져서 설계범위를 좁혀봅시다. <br>
하단의 5개 항목들은  `면접관 - 면접자 질문 💬` 으로 실제 얻어낸 정보들입니다.

* 지리적으로 `가깝다` 라는 것의 기준: 두 사용자 간의 직선 거리 5km
* 10억명을 가정하고, 그 가운데 10% 정도가 해당 기능을 사용
* 사용자 이동 이력 보관 여부 - 기계 학습 등 여러가지 용도로 활용될 수 있으므로, Yes
* 친구 관계에 있는 사용자가 10분 이상 비활성화 상태면, 해당 사용자를 주변 친구 목록에서 사라지도록 한다.
* 사생활 및 데이터 보호법 고려 - 일단 설계단계에서는 생략

### 기능적 요구사항 ⚒️
* 사용자는 모바일 앱에서 주변 친구를 확인할 수 있어야 한다.
  * 주변 친구 목록에 보이는 각 항목에는 해당 친구까지의 거리, 해당 정보가 마지막으로 갱신된 시각(`timestamp`) 가 함께 표시되어야 한다.
  * 몇 초마다 한 번씩 주변 친구 목록이 갱신되어야 한다.
 
### 비기능적 요구사항 🥳
* 낮은 지연시간(low latency): 위치 변화가 반영되는 데 너무 오랜 시간이 걸리지 X
* 안정성: 전반적으로 안정적이어야 하지만, 때로는 몇 개 데이터가 유실되는 것은 용인 가능
* 결과적 일관성(eventual consistency): 위치 데이터를 저자하기 위해, 강한 일관성을 지원하는 데이터 저장소를 사용할 필요는 없다. → 복제본의 데이터가 원본과 동일하게 변경되기까지 몇 초 정도 걸리는 것은 용인할 수 O


 해당 정보들을 바탕으로, 제약사항과 가정들을 정리해봅시다.

1. `주변 친구` 는 5마일(8km) 반경 이내의 친구로 정의합니다.
2. 친구 위치 정보는 30초 주기로 갱신합니다. (사람이 걷는 속도 고려)
3. 평균적으로 매일 주변 친구 검색 기능을 활용하는 사용자는 1억 명으로 가정합니다.
4. 동시 접속 사용자의 수는 DAU 수의 10%로 가정합니다. → 1억 명의 10%인 1000만명이 동시에 시스템을 사용한다고 가정합니다.
5. 평균적으로 한 사용자는 400명의 친구를 갖는다고 가정합니다. 그리고 그 모두가 주변 친구 검색 기능을 활용한다고 가정합니다.

**QPS 계산** <br>
1억 DAU<br>
동시 접속 사용자: 10% * 1억 = 1000만<br>
사용자는 30초마다 자기 위치를 시스템에 전송 <br>
위치 정보 계산 QPS = 천만 / 30 =~ 334,000<br>

## 2단계. 개략적 규모 추정
이제 개략적 설계, API 설계, 데이터 모델에 관련한 내용을 살펴봅시다.<br>
이 장에서 해결하는 문제의 경우, 위치 정보를 모든 친구에게 전송(PUSH)해야 한다는 이슈 때문에 클-서 사이의 통신이 **단순 HTTP 프로토콜을 사용하지 못하게 될 수 있음**을 감안합니다.<br>
따라서, 개략적 설계안부터 차근차근 살펴봅시다. 🥳<br>

### 개략적 설계안
**메시지의 효과적 전송을 가능**하게 할 설계안을 요구합니다.<br>
개념적으로 보면, 사용자는 근방의 모든 활성 상태 친구의 새 위치 정보를 수신하고자 합니다. <br>
이론적으로는 순수한 P2P(Peer-To-Peer) 방식으로도 해결 가능한 문제입니다.<br></br>
모바일 단말은 통신 연결 상태가 좋지 않은 경우도 있고, 사용할 수 있는 전력도 충분하지 않아서 실용적인 아이디어는 아니지만, 이를 통해 일반적으로 추구해야 할 설계 방향에 대한 통찰은 얻을 수 있겠습니다.
<br>
이보다 조금 더 실용적인 설계안은, <u>**공용 백엔드를 사용**</u>하는 것입니다.
![image](https://github.com/user-attachments/assets/34f9d102-a030-4a7a-9634-452b702af84e)

<br></br>
공용 백엔드의 역할로는,
* 모든 활성 상태 사용자의 위치 변화 내역을 수신한다.
* 사용자 위치 변경 내역을 수신할 때마다 해당 사용자의 모든 활성 상태 친구를 찾아서, 그 친구들의 단말로 변경 내역을 전달한다.
* 두 사용자 사이의 거리가 특정 임계치보다 먼 경우에는, 변경 내역을 전송하지 않는다.
→ 334,000 QPS `*` 400 `*` 10% = 1400만 건의 위치 정보 갱신 요청을 견디기 힘들다는 단점이 있습니다.
<br> </br>

### 설계안
소규모 백엔드를 위한 개략적 설계안부터 만들어봅시다.
![image](https://github.com/user-attachments/assets/848ff47b-2b7b-48f7-8e4d-beaf086bd36a)

컴포넌트적 관점
---
**로드 밸런서**</br>
로드 밸런서는 RESTful 서버 및 양방향 유상태(stateful) 웹 소켓 서버 앞단에 위치합니다.
- RESTful API 서버는 무상태(stateless) API 서버의 클러스터로서, 친구를 추가/삭제하거나, 사용자 정보를 갱신하는 등의 부가적인 작업을 처리합니다. 
- 웹 소켓 서버는 친구 위치 정보 변경을 거의 실시간에 가깝게 처리하는 유상태 서버 클러스터입니다. 각 클라이언트는 그 가운데 한 대와 웹 소켓 서버를 지속적으로 유지하여야 합니다.

**레디스 위치 정보 캐시**
- 활성 상태 사용자의 가장 최근 위치 정보를 캐시하는 데 사용됩니다. (TTL 필드로, 기간이 지나면 해당 사용자는 비활성 상태로 바뀌고 위치 정보는 캐시에서 삭제됩니다.)

**사용자 데이터베이스**
- 사용자 데이터 및 친구 관계 정보 저장

**위치 이동 이력 데이터베이스**
- 사용자의 위치 변동 이력 보관

**레디스 펍/섭(Pub/Sub) 서버**
- ![image](https://github.com/user-attachments/assets/f457df57-1195-4623-957c-fb6c2fb518de)
- 웹 소켓 서버를 통해 수신한 특정 사용자의 위치 정보 변경 이벤트는 해당 사용자에게 배정된 Pub/Sub 채널에 발행합니다.

주기적 위치 갱신
---
어떤 사용자의 위치가 바뀌었을 때 어떤 일이 일어나는지 시스템적 관점에서 살펴봅시다.

1. 모바일 클라이언트가 위치가 변경된 사실을 로드밸런서에 전송합니다.
2. 로드밸런서는 그 위치 변경 내역을 해당 클라이언트와 웹소켓 서버 사이에 설정된 연결을 통해 웹소켓 서버에 보냅니다.
3. 웹소켓 서버는 해당 이벤트를 위치 이동 이력 데이터베이스에 저장합니다.
4. 웹소켓 서버는 새 위치를 위치 정보 캐시에 보관합니다. (TTL 새로 갱신)
5. 웹소켓 서버는 레디스 Pub/Sub 서버의 해당 사용자 채널에 새 위치를 발행합니다. (3-5단계: 병렬 수행)
6. 웹소켓 이벤트 핸들러(=위치 변경 이벤트를 보낸 사용자의 온라인 상태 친구들) 에게 브로드캐스트(broadcast)됩니다.
7. 메시지를 받은 웹소켓 서버(=웹소켓 연결 핸들러가 위치한 웹소켓 서버)는 새 위치를 보낸 사용자-메시지를 받은 사용자 사이의 거리를 새로 계산합니다.
8. 만약 7에서 계산한 거리가 검색 반경을 넘지 않는다면, 새 위치 및 해당 위치로의 이동이 발생한 시각을 나타내는 타임스탬프를 해당 구독자의 클라이언트 앱으로 전송합니다.

![image](https://github.com/user-attachments/assets/9aa6d31b-81aa-4673-b67f-d766ddeba57f)

1. 사용자 1의 위치가 변경되면, 변경 내역은 사용자 1과 연결을 유지하는 웹소켓 서버에 전송됩니다.
2. 해당 변경 내역은 레디스 펍/섭(Pub/Sub) 서버 내의 사용자 1 채널로 발행됩니다.
3. 레디스 펍/섭(Pub/Sub) 서버는 해당 변경 내역을 모든 구독자에게 브로드캐스트합니다. (구독자= 사용자 1과 친구 관계에 있는 모든 웹소켓 핸들러)

API 설계 
---
* **웹소켓**: 사용자는 웹소켓 프로토콜을 통해 위치 정보 내역을 전송/수신합니다.

**1. 서버 API: 주기적인 위치 정보 갱신**
- Request: 위도, 경도, 시각 정보 전송
- Response: X

**2. 클라이언트 API: API 클라이언트가 갱신된 친구 위치를 수신하는 데 사용할 API**
- 전송 데이터: 친구 위치 데이터와 변경된 시각을 나타내는 타임스탬프

**3. 서버 API: 웹소켓 초기화 API**
- Request: 위도, 경도, 시각 정보 전송
- Response: 자기 친구들의 위치 데이터 수신

**4. 클라이언트 API: 새 친구 구독 API**
- Request: 웹소켓 서버- 친구 ID 전송
- Response: 가장 최근의 위도, 경도, 시각 정보 전송

**5. 클라이언트 API: 구독 해지 API**
- Request: 웹소켓 서버 - 친구 ID 전송
- Response: X

* **HTTP 요청**

데이터 모델 
---
위치 정보 캐시 및 위치 이동 이력 데이터베이스를 살펴보도록 합시다.

### 위치 정보 캐시
![image](https://github.com/user-attachments/assets/a193810e-707f-4a65-a3ea-2febdfe1bc2d) </br>
`주변 친구` 기능을 켠 활성 상태의 친구의 가장 최근 위치를 보관합니다. </br>
위치 정보 저장에 데이터베이스를 사용하지 않는 이유는, 사용자의 **현재 위치**만을 사용하기 때문입니다. (=사용자 위치는 하나만 저장하면 충분합니다)
따라서, **영속성을 보장할 필요가 없다**는 사실에 유념합시다.

### 위치 이동 이력 데이터베이스
![image](https://github.com/user-attachments/assets/e777806b-369a-4029-9507-e7c94035d75a) 
위치 이동 이력 데이터베이스는 다음 스키마를 따르는 테이블에 저장합니다. </br>
카산드라(Cassandra)는 막대한 쓰기 연산 부하를 감당할 수 있고, 수평적 규모 확장이 가능한 데이터베이스입니다. </br>
관계형 DB를 이용하기에는 이력 데이터의 양이 `user_id`를 기준으로 한 샤딩이 필요할 수도 있습니다.


## 3. 상세 설계
이번 절에서는 앞선 개략적 설계안에서 규모를 늘려나가면서 병목과 해결책을 고안해내는 데 집중해봅시다.

### 중요 구성요소별 규모 확장성
* **API 서버**
  해당 설계안의 API 서버는 무상태 서버로, 클러스터 규모를 CPU Utiliztion이나 부하, I/O 상태에 따라 자동으로 늘리는 방법은 다양하기 때문에 특별히 다루지는 않겠습니다.
* **웹소켓 서버**
  웹소켓 서버는 유상태 서버이기 때문에 기존 서버를 제거할 때는 약간 주의를 기울입시다. → 노드를 제거하기 전에 기존 연결부터 종료될 수 있도록 해야합니다. ('좋은' 로드밸런서가 인식하는 노드 상태를 `연결 종료 중(draining)`으로 변경해둡니다.) 모든 연결이 종료되면, 서버를 제거합니다.
* **클라이언트 초기화**
   `지속성 연결` 에 해당하는 웹소켓 연결이 초기화되면, 클라이언트는 해당 모바일 단말을 이용 중인 사용자의 위치 정보를 전송합니다. 그 정보를 받은 연결 핸들러는 다음 작업을 수행합니다.
  1. 위치 정보 캐시에 보관된 해당 사용자의 위치를 갱신하고, 연결 핸들러 내의 변수에 저장해둡니다.
  2. 사용자 DB를 뒤져 해당 사용자의 모든 친구 정보를 가져옵니다.
  3. 위치 정보 캐시에 일괄(batch) 요청을 보내어 모든 친구의 위치를 한 번에 가져옵니다.
  4. 캐시가 가져온 친구 위치 각각에 대해, 웹소켓 서버는 해당 친구와 사용자 사이의 거리를 계산합니다. 반경 이내이면 상세 정보, 위치, 타임스탬프를 웹소켓 연결을 통해 클라이언트에 반환합니다.
  5. 웹소켓 서버는 각 친구의 레디스 펍/섭(Pub/Sub) 채널을 구독합니다.
  6. 사용자의 현재 위치를 레디스 펍/섭(Pub/Sub) 서버의 전용 채널을 통해 모든 친구에게 전송합니다.
 * **사용자 데이터베이스**
    2가지 종류의 데이터가 보관됩니다. (한 대의 RDB서버로는 감당 X, `user_id` 기준 샤딩이면 수평적 규모 확장 가능)
   1. 사용자 상세 정보 - 사용자 ID, 사용자명, 프로파일 이미지 URL
   2. 친구 관계 데이터
* **위치 정보 캐시**
   레디스 서버 한 대로 1000만 명의 활성 사용자가 30s마다 변경된 위치 정보를 전송하면, 고사양 서버라 해도 부담되는 수치일 수 있지만, 요 경우에는 캐시 데이터를 쉽게 샤딩할 수 있습니다.
  가용성을 높이려면 ,각 샤드에 보관하는 위치 정보를 대기(stand-by) 노드에 복제둡시다. 그러면 primary 노드에 장애가 발생해도 stand-by 노드를 승격시켜 발빠르게 대처할 수 있습니다.
* **레디스 펍/섭 서버**
  펍/섭 서버를 **모든 온라인 친구에게 보내는 위치 변경 내역 메시지의 라우팅(routing) 계층으로 활용**합니다. (구독자 관계 추적 - 해시 테이블(hash table), 연결 리스트(linked list) → 소량의 메모리만을 사용)
  1. `주변 친구` 기능을 활용하는 모든 사용자에게 채널 하나 씩을 부여합니다. 초기화 시에, 모든 친구의 채널과 구독 관계를 설정합니다.
  2. 더 많은 메모리를 사용하게 되지만, 메모리가 병목이 될 가능성은 낮다고 합니다. 

 * 레디스 펍/섭 서버의 병목은 메모리가 아니라 CPU 사용량입니다.
 * 우리가 현재 풀어야 할 문제의 규모를 감당하려면, 분산 레디스 펍/섭 클러스터가 필요합니다.
**분산 레디스 펍/섭 서버 클러스터**
수백 대의 레디스 서버에 채널을 어떻게 분산할 것인가요? 메시지를 발행할 `user_id`를 기준으로 펍/섭 서버를 단순하게 샤딩하면 되는 걸까요?
그러나, 현실적으로는 수백 대의 펍/섭 서버가 관련되어있는 문제이기 때문에 동작 방식을 좀 더 깊게 들어가봅시다.

해당 설계안에서는 서비스 탐색(service discovery) 컴포넌트를 도입하여 문제를 해결합니다.
`etcd`, `주키퍼(ZooKeeper)` 등이 가장 널리 활용된다고 합니다.

1. 가용한 서버 목록을 나열하는 기능 및 해당 목록을 갱신하는 데 필요한 UI, API, 서비스 탐색 SW는 설정을 보관하기 위한 소규모의 키-값 저장소라고 보시면 됩니다.
   * 키: `/config/pub_sub_ring`
   * 값: `["p_1", "p_2", "p_3", "p_4]`
2. 클라이언트로 하여금 '값'에 명시된 레디스 펍/섭 서버에 발생한 변경 내역을 구독할 수 있도록 하는 기능입니다.

1번에 해당 하는 '키'에 매달린 '값'에는 활성사애의 모든 레디스 펍/섭 서버로 구성된 해시 링을 보관합니다.
레디스 펍/섭 서버는 메시지를 발행할 채널이나 구독할 채널을 정해야할 때 해당 해시 링을 참조합니다.

다음은 웹소켓 서버가 특정 사용자 채널에 위치 정보 내역을 발행하는 과정이 어떻게 처리되는지를 보여줍니다.
![image](https://github.com/user-attachments/assets/db224e14-26c3-4df2-bf62-36474cd20487)
1. 웹소켓 서버는 해시링을 참조 → 메시지를 발행할 레디스 펍/섭 서버를 선정
   - 정확한 정보는 서비스 탐색 컴포넌트에 보관돼있으, 성능 상 웹소켓 서버에 캐시하는 것도 생각 가능
2. 웹소켓 서버는 해당 서버가 관리하는 사용자 채널에 위치 정보 변경 내역을 발행합니다.
### 레디스 펍/섭 서버 클러스터의 확장 고려

레디스의 Pub/Sub은 메시지를 디스크나 메모리에 영속적으로 저장하지 않습니다. 구독자가 없다면 메시지는 바로 소멸되며, 전송 이후에도 따로 저장되지 않기 때문에 무상태(stateless) 데이터라고 볼 수 있습니다.

하지만 각 채널의 구독자 목록은 서버가 기억하고 있어야 하며, 이 정보는 상태(state)로 관리됩니다. 즉, Pub/Sub 서버는 무상태 메시징을 하더라도, 실제 운영 관점에서는 유상태 서버로 다뤄야 합니다.

서버를 추가하거나 교체할 때는 이 구독자 정보를 기반으로 채널을 재배치하거나, 재구독 요청을 클라이언트에 전달해야 합니다. 단순히 서버 수만 늘리는 방식으로는 확장이 어렵고, Pub/Sub 서버 클러스터는 운영에 주의가 필요합니다.

### 클러스터 사이즈 조정 시 고려할 점

유상태 클러스터는 단순한 수평 확장만으로 해결되지 않기 때문에, 대부분 트래픽보다 여유 있게 클러스터를 구성해 놓고, 가능한 한 변경을 자제하는 방향으로 운영합니다. 확장이 필요하다면, 트래픽이 적은 시간대에 수행하는 것이 좋습니다.

채널이 다른 서버로 이동하면 기존 구독자들에게 재구독 요청이 발생하기 때문에, 이 작업은 순간적으로 부하를 유발할 수 있습니다.

### 확장 절차 예시

1. 해시 링에 포함된 서버 수를 조정합니다.
2. 새로운 서버를 추가합니다.
3. 해시 링 구성을 갱신합니다.
4. 모든 웹소켓 서버는 변경된 링 정보를 참조하여, 구독 중인 채널들의 재구독을 시도합니다.

예를 들어, 기존에 `["p_1", "p_2", "p_3", "p_4"]`로 구성되어 있던 링을 `["p_1", "p_2", "p_3", "p_4", "p_5", "p_6"]`로 확장할 수 있습니다.

### 서버 교체가 필요한 경우

장애로 인해 특정 서버를 교체해야 할 경우, 예를 들어 `p_1` 서버를 `p_1_new`로 교체한다면, 해당 서버가 담당하고 있던 채널에 대한 구독 정보를 새로운 서버로 넘겨야 합니다.

서비스 탐색 컴포넌트는 변경된 해시 링 정보를 웹소켓 서버들에게 전달하고, 웹소켓 서버는 각자의 연결 핸들러가 구독 중이던 채널들을 새로 구독하도록 요청합니다.

### 친구 추가/삭제 시 처리 흐름

사용자가 친구를 추가하면, 해당 친구의 위치 정보 채널을 새로 구독합니다. 이때 가장 최근 위치와 타임스탬프를 함께 수신하게 됩니다.

반대로 친구를 삭제하면, 해당 친구 채널의 구독을 해지합니다. 이는 단순한 친구 삭제뿐만 아니라, 위치 정보 공유 허용/해제 같은 기능에도 재사용할 수 있는 구조입니다.

### 친구 수가 많은 경우

사용자가 친구를 많이 보유한 경우에도 걱정할 필요는 없습니다. 예를 들어 친구가 5,000명인 경우, 각 친구 채널은 여러 펍/섭 서버에 분산되어 구독됩니다. 하나의 서버에 부하가 몰리는 일은 상대적으로 드뭅니다.

다만, 특정 소수 사용자에게 많은 친구가 몰려 있다면, 일부 펍/섭 서버에 구독 요청이 집중될 수 있으므로 충분한 수의 서버를 확보해두는 것이 좋습니다.

### 친구가 아닌 주변 사용자의 위치 정보

친구가 아닌 사용자와의 거리 정보를 표시하려면, 지오해시(GeoHash)를 활용한 채널 구독 방식이 필요합니다. 각 위치는 일정 격자로 나뉘고, 해당 위치에 기반한 지오해시 채널이 설정됩니다.
![image](https://github.com/user-attachments/assets/95b224be-20dc-44b7-8d80-4d72ba1a1b72)

사용자는 본인이 위치한 격자 하나와, 인접한 8개의 격자를 포함하여 총 9개의 지오해시 채널을 구독하게 됩니다. 위치가 바뀌면 새로운 지오해시 채널로 메시지를 발행하고, 같은 채널을 구독하고 있는 사용자에게 알림을 보냅니다.
![image](https://github.com/user-attachments/assets/df43ba9f-f585-4bf8-8d2c-c88c0c6376bb)

이 방식을 사용하면 친구가 아니더라도, 일정 거리 이내의 사용자 위치를 실시간으로 알릴 수 있습니다.

### Erlang 기반 메시징 시스템

레디스 기반 펍/섭 시스템 외에도 Erlang을 사용하는 메시징 시스템도 고려할 수 있습니다.

Erlang은 수십만 개의 초경량 프로세스를 효율적으로 다룰 수 있는 언어이며, OTP(오픈 텔레콤 플랫폼)를 통해 장애 대응, 메시지 전달, 로깅 등 다양한 기능을 제공합니다. 사용자를 각각 Erlang 프로세스로 매핑하여, 위치 정보를 친구 프로세스에 메시지로 전파하는 방식으로 설계할 수 있습니다.

이런 구조는 메시지 전달량이 많고 지연 시간이 짧아야 하는 시스템에서 성능상 장점이 있지만, 팀 내에 Erlang에 대한 이해도가 필요하다는 점은 단점으로 작용할 수 있습니다.
   

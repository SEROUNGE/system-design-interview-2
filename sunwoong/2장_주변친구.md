주변 친구는 앱 사용자 가운데 본인 위치 정보 접근 권한을 허락한 사용자에 한해 인근의 친구 목록을 보여주는 시스템이다.

### 1단계. 문제 이해 및 설계 범위 확정

**기능 요구사항**

- 사용자는 모바일 앱에서 주변 친구를 확인할 수 있다. 해당 친구까지의 거리, 정보가 마지막으로 갱신된 시각이 함께 표시되어야 한다.
- 친구 목록은 몇 초마다 한 번씩 갱신되어야 한다.

**비기능 요구사항**

- 낮은 지연 시간
- 안정성
- 결과적 일관성
    - 위치 데이터를 저장하기 위해 강한 일관성을 지원하는 데이터 저장소를 사용할 필요는 없다.

**개략적 규모 추정**

- 주변 친구는 5마일(8km) 반경 이내 친구로 정의
- 친구 위치 정보는 30초 주기로 갱신
- 매일 주변 친구 검색 기능을 활용하는 사용자는 1억명
- 동시 접속 사용자 수는 DAU의 10% 즉, 1000만명
- 한 사용자는 400명의 친구를 갖는다고 가정
- 이 기능을 제공하는 앱은 페이지당 20명의 주변 친구를 표시

### 2단계. 개략적 설계안 제시 및 동의 구하기

**<개략적 설계안>**

활성 상태인 근방 모든 친구와 항구적 통신 상태를 유지하면 된다.

![1.png](attachment:bd0855c0-0dee-43ad-8505-4767b1be4d49:1.png)

**로드밸런서**

RESTful API 서버 및 양방향 유상태 웹소켓 서버 앞단에 위치한다. 부하를 고르게 분산하기 위해 트래픽을 서버들에 배분하는 역할을 한다.

**RESTful API 서버**

무상태 API 서버의 클러스터로서, 통상적인 요청/응답 트래픽을 처리한다.

**웹소켓 서버**

친구 위치 정보 변경을 거의 실시간에 가깝게 처리하는 유상태 서버 클러스터다. 각 클라이언트는 한 대의 서버와 웹소켓 연결을 지속적으로 유지한다.

검색 반경 내 친구 위치가 변경되면 해당 내역은 이 연결을 통해 클라이언트로 전송된다.

**레디스 위치 정보 캐시**

레디스는 활성 상태 사용자의 가장 최근 위치 정보를 캐시하는 데 사용한다. TTL 필드를 사용해서 기간이 지나면 해당 사용자는 비활성 상태로 바뀌고 그 위치 정보는 캐시에서 삭제된다. 캐시에 보관된 정보를 갱신할 때는 TTL도 갱신한다.

**사용자 데이터베이스**

사용자 데이터베이스에는 사용자 데이터 및 사용자의 친구 관계 정보를 저장한다.

**위치 이동 이력 데이터베이스**

사용자의 위치 변동 이력을 보관한다.

**레디스 pub/sub 서버**

![2.png](attachment:41d947ce-7d60-4ca6-a883-96a21041f670:2.png)

레디스 pub/sub은 초경량 메시지 버스다. 새로운 채널을 생성하는 것은 아주 값싼 연산이며 GB급 메모리를 갖춘 최신 레디스 서버에는 수백만 개의 채널을 생성할 수 있다.

본 설계안에서는 웹소켓 서버를 통해 수신한 특정 사용자의 위치 정보 변경 이벤트는 해당 사용자에게 배정된 pub/sub 채널에 발행한다. 해당 사용자의 친구 각각과 연결된 웹소켓 연결 핸들러는 해당 채널의 구독자로 설정되어 있다. 따라서 특정 사용자의 위치가 바뀌면 해당 사용자의 모든 친구의 웹소켓 연결 핸들러가 호출된다. 각 핸들러는 위치 변경 이벤트를 수신할 친구가 활성 상태면 거리를 다시 계산하고 해당 거리가 검색 반경 이내면 갱신된 위치와 갱신 시각을 웹소켓 연결을 통해 해당 친구의 클라이언트 앱으로 보낸다.

**<주기적 위치 갱신>**

모바일 클라이언트는 항구적으로 유지되는 웹소켓 연결을 통해 주기적으로 위치 변경 내역을 전송함.

1. 모바일 클라이언트가 위치가 변경된 사실을 로드밸런서에 전송
2. 로드밸런서는 그 위치 변경 내역을 웹소켓 서버로 보냄
3. 웹소켓 서버는 해당 이벤트를 위치 이동 이력 데이터베이스에 저장
4. 웹소켓 서버는 새 위치를 위치 정보 캐시에 저장. 이때 TTL도 갱신하고 웹소켓 연결 핸들러 안의 변수에 해당 위치를 반영 (추구 거리 계산에 이용됨)
5. 웹소켓 서버는 레디스 pub/sub 서버의 해당 사용자 채널에 새 위치를 발행함. (3 ~ 5, 병렬 처리)
6. 레디스 pub/sub 채널에 발행된 새로운 위치 변경 이벤트는 모든 활성 상태 구독자에게 브로드캐스트.
7. 메시지를 받은 웹소켓 서버는 거리를 계산함. 검색 반경 이내인 경우, 새 위치 및 해당 위치로의 이동이 발생한 시각을 해당 구독자의 클라이언트 앱으로 전송

**<데이터 모델>**

**위치 정보 캐시**

위치 정보 캐시는 주변 친구 기능을 켠 활성 상태 친구의 가장 최근 위치를 보관한다. 레디스를 사용해 캐시를 구현하며 사용자 ID : {위도, 경도, 시각} 키, 값을 가진다.

위치 정보 저장에 데이터베이스를 사용하지 않는 이유는?

주변 친구 기능은 사용자의 현재 위치만을 이용한다. 따라서 사용자 위치는 하나만 보관하면 충분하다. 레디스는 읽기, 쓰기 연산 속도가 매우 빠르고 TTL을 지원하므로 적합한 저장소이다. 주변 친구 기능이 활용하는 위치 정보에 대해서는 영속성을 보장할 필요가 없기 때문이기도 하다. 레디스 서버 하나에 장애가 발생하면 다른 새 서버로 바꾼 다음 갱신된 위치 정보가 캐시에 채워지기를 기다리면 충분하다.

**위치 이동 이력 데이터베이스**

사용자의 위치 이동 정보 변경 이력을 {유저 ID, 위도, 경도, 시각} 스키마를 가진 테이블에 저장한다.

### 3단계. 상세 설계

**레디스 pub/sub 서버**

모든 온라인 친구에게 보내는 위치 변경 내역 메시지의 라우팅 계층으로 활용한다. 채널을 만드는 비용이 아주 저렴하다. 새 채널은 구독하려는 채널이 없을 때 생성하고 구독자가 없는 채널로 전송된 메시지는 버려진다. 채널 하나를 유지하기 위해서는 구독자 관계를 추적하기 위한 해시 테이블과 연결 리스트가 필요한데 아주 소량의 메모리만을 사용한다. 오프라인 사용자라 어떤 변경도 없는 채널의 경우에는 생성된 이후에 cpu 자원을 전혀 사용하지 않는다.

레디스 pub/sub 서버를 사용하는 경우, cpu 사용량에 따른 병목을 고려해야 한다. 따라서 본 설계안의 규모를 감당하기 위해 분산 레디스 pub/sub 클러스터가 필요하다.

**분산 레디스 pub/sub 클러스터**

모든 채널은 서로 독립적이므로 메시지를 발행할 사용자 ID를 기준으로 pub/sub 서버들을 샤딩하면 된다. 그러나 서버에는 필연적으로 장애가 생기게 마련이므로, 운영을 매끄럽게 하려면 좀 더 고민이 필요하다.

본 설계안에서는 서비스 탐색 컴포넌트를 도입하여 문제를 해결한다. 서비스 탐색 이용 방식은 기본적이며 아래와 같다.

1. 가용한 서버 목록을 유지하는 기능 및 해당 목록을 갱신하는 데 필요한 UI나 API. 서비스 탐색 소프트웨어는 설정 데이터를 보관하기 위한 소규모의 키-값 저장소라고 보면 된다.
    1. 키: /config/pub_sub_ring
    2. 값: [”p_1”, “p_2”, “p_3”, “p_4”] (활성 상태의 모든 레디스 pub/sub 서버로 구성된 해시 링
2. 클라이언트로 하여금 ‘값’에 명시된 레디스 pub/sub 서버에서 발생한 변경 내역을 구독할 수 있도록 하는 기능.

레디스 pub/sub 서버는 메시지를 발행할 채널이나 구독할 채널을 정해야 할 때 이 해시 링을 참조한다.

1. 웹소켓 서버는 해시 링을 참조하여 메시지를 발행할 레디스 pub/sub 서버를 선정한다. 정확한 정보는 서비스 탐색 컴포넌트에 보관되어 있으나 성능 효율을 높이고 싶다면 해시 링 사본을 웹소켓 서버에 캐시하는 것도 생각해 볼 수 있다.
2. 웹소켓 서버는 해당 서버가 관리하는 사용자 채널에 위치 정보 변경 내역을 발행한다.

**레디스 pub/sub 서버 클러스터의 규모 확장 고려사항**

레디스 pub/sub 서버 클러스터의 속성은?

- pub/sub 채널에 전송되는 메시지는 메모리나 디스크에 지속적으로 보관되지 않는다. 채널의 모든 구독자에게 전송되고 나면 바로 삭제된다. 구독자가 없는 경우는 그냥 지워진다. 이런 관점에서 보면 pub/sub 채널을 통해 처리되는 데이터는 무상태라고 할 수 있다.
- pub/sub 서버는 채널에 대한 상태 정보를 보관한다. 각 채널의 구독자 목록이 핵심이며 특정한 채널을 담당하던 pub/sub 서버를 교쳬하거나 해시 링에서 제거하는 경우 채널을 다른 서버로 이동시켜야 하고 해당 채널의 모든 구독자에게 그 사실을 알려야 한다. 이런 관점에서 보면 pub/sub은 유상태 서버다.

이러한 속성들로 레디스 pub/sub 서버 클러스터는 유상태 서버 클러스터로 취급하는 것이 바람직하다. 유상태 서버 클러스터의 규모를 늘리거나 줄이는 것은 운영 부담과 위험이 큰 작업이다. 유상태 서버 클러스터는 혼잡 시간대 트래픽을 감당하고 불필요한 크기 변화를 피할 수 있도록 오버 프로비저닝하는 것이 보통이다.

불가피하게 규모를 확장하기 위해선 아래와 같은 문제가 발생할 수 있음을 유의해야 한다.

- 클러스터의 크기를 조정하면 많은 채널이 같은 해시 링 위의 다른 여러 서버로 이동한다. 서비스 탐색 컴포넌트가 모든 웹소켓 서버에 해시 링이 갱신되었음을 알리면 엄청난 재구독 요청이 발생할 것이다.
- 재구독 요청을 처리하다 보면 클라이언트가 보내는 위치 정보 변경 메시지의 처리가 누락될 수 있다.
- 서비스의 상태가 불안정해질 가능성이 있으므로 클러스터 크기 조정은 하루 중 시스템 부하가 가장 낮은 시간을 골라서 시행해야 한다.

클러스터 크기는 어떻게 조정하는가?

- 새로운 링 크기를 계산한다. 크기가 늘어나는 경우 새 서버를 준비한다.
- 해시 링의 키에 매달린 값을 새로운 내용으로 갱신한다.
- 대시보드를 모니터링한다. 웹소켓 클러스터의 CPU 사용량이 어느 정도 튀는 것이 보여야 한다.

**주변의 임의 사용자 기능이 추가된다면?**

위치 정보 공유에 동의한 주변 사용자를 무작위로 보여 줄 수 있는 기능을 추가해보자. 한 가지 방법으로는 지오해시에 따라 구축된 pub/sub 채널 풀을 두는 것이다. 즉, 특정 격자 내의 모든 사용자는 해당 격자에 할당된 채널을 구독한다.

격자 9q8znd가 있다고 가정하겠다.

1. 사용자 2의 위치가 변경되면 웹소켓 연결 핸들러는 해당 사용자의 지오해시 ID를 계산한 다음, 해당 지오해시 ID를 담당하는 채널에 새 위치를 전송한다.
2. 근방에 있는 사용자 가운데 해당 채널을 구독하고 있는 사용자는 사용자 2의 위치가 변경되었다는 메시지를 수신한다.

격자 경계 부근에 있는 사용자를 잘 처리하기 위해 모든 클라이언트는 사용자가 위치한 지오해시뿐 아니라 주변 지오해시 격자를 담당하는 채널도 구독한다.

**카산드라 Eventually Consistency (최종 일관성)**

데이터베이스 시스템에서 모든 복제본이 언젠가는 일관된 상태에 도달할 것임을 보장하는 일관성 모델이다. 카산드라에서는 다음과 같은 특성을 가진다.

- 비동기 복제
    - 쓰기 작업을 수행한 후 모든 복제본에 대해 즉시 일관성을 보장하지 않는다. 대신, 비동기적으로 복제가 이루어진다. 이는 쓰기 작업이 클러스터 전체에 전파되는 시간을 의미한다.
    - 쓰기 작업의 응답 시간을 줄이는데 기여한다. 따라서 클라이언트는 쓰기 작업이 성공한 즉시 응답을 받고 다음 작업을 진행할 수 있다.
- 읽기 작업의 일관성 보장
    - 쓰기 작업 후 모든 복제본이 동일한 데이터를 가지게 될 때까지 시간이 걸릴 수 있지만, 읽기 작업에 대해서는 일관성을 보장한다. 즉, 최근에 쓰여진 데이터가 아니더라도 이전에 쓰인 데이터는 정확하게 읽을 수 있다.
- 일관성 수준 커스텀
    - 일관성 수준을 조절할 수 있는 옵션을 제공한다. 한 노드에만 쓰기를 완료하면 성공(’ONE’)에서부터 노드의 과반수 이상에 쓰기가 완료되어야 성공(’QUORUM’) 등 다양하게 설정 가능하다.
- 분산 환경에서의 적용
    - 다중 데이터 센터 환경에서도 최종 일관성 모델을 유지하며, 데이터의 지역적 복제와 장애 복구 기능을 통해 데이터의 가용성과 일관성을 동시에 유지할 수 있다.

**레디스 pub/sub 서버와 메시지 버스(카프카/메시지 큐)의 작동 방식을 비교해 보자.**

기존 작동 방식

- 레디스 pub/sub 서버
    - 레디스는 pub/sub 방식으로 메시지를 전송함.
    - 발행자가 메시지를 채널에 발행하면 구독자가 그 채널을 통해 메시지를 받음
    - 브로드캐스트를 하지 않으면 특정 채널을 구독 중인 구독자에게만 메시지가 전달됨
- 메시지 버스(카프카/메시지 큐)
    - 카프카와 메시지 큐는 메시지를 토픽 기반으로 전송함
    - 생산자가 메시지를 토픽에 전송하면 소비자가 그 토픽을 구독하고 메시지를 받음
    - 브로드캐스트 없이도 특정 토픽을 구독 중인 소비자에게만 메시지가 전달됨

메시지 되감기

- 레디스 pub/sub 서버
    - 메시지를 발행하면 되감기가 불가능
    - 구독자가 메시지를 놓치면 다시 받을 수 없음
- 카프카
    - 메시지 되감기 가능
    - 소비자는 특정 오프셋으로 되돌아가 이전 메시지를 다시 읽을 수 있음

확장성

- 레디스 pub/sub 서버
    - 다수의 구독자가 있을 때, 성능 저하와 같은 문제가 발생할 수 있음
        - 단일 스레드로 동작하기 때문에 메시지 분배 비용 증가
        - 모든 구독자에게 실시간으로 메시지를 레디스가 직접 푸시하기 때문에 한 구독자의 응답 속도나 네트워크 지연이 전체 처리 속도에 영향을 줄 수 있음
        - pub/sub 채널은 레디스 클러스터에서도 파티셔닝되지 않아 스케일 아웃이 불가능
        - 버퍼 오버플로우 가능성
- 카프카
    - 브로커와 소비자만 늘리면 확장이 용이함
    - 리밸런싱이나 메시지가 소비되지 않는 경우 장애가 발생할 수 있지만 이런 문제를 관리하는 도구와 기능이 제공됨
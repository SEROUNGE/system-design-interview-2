이번 장에서는, 모바일 앱의 ‘주변 친구’ 기능을 제공하는 시스템을 설계해본다. 주변 친구 기능은 사용자의 위치 정보를 기반으로, 현재 주변에 있는 친구의 목록을 보여주는 기능이다.
_물론 위치 정보 접근 권한을 허용한 사용자에 한해 제공된다._

이전 장의 ‘근접성 서비스’와 다른 점은, 사업장의 위치는 정적이지만 사용자들의 위치는 자주 변경될 가능성이 높다는 점이다. 이 차이점에 주목하여 시스템 설계를 진행한다.

# Redis Pub/Sub

## Pub/Sub

![redis-6-4.jpeg](attachment:006eaff3-8f9b-4832-aa17-731aefd5f265:redis-6-4.jpeg)

레디스 pub/sub은 아주 가벼운 메시지 브로커로, 레디스 노드에 접근할 수 있는 모든 클라이언트가 발행자 및 구독자가 될 수 있다. pub/sub에서 발행자는 특정 채널에 메시지를 보내고, 구독자는 특정 채널을 리스닝하면서 메시지를 읽어가게 된다.

_redis pub/sub으로 채널을 생성하는 것은 매우 적은 비용이 들며, GB 단위의 메모리를 가진 redis 서버라면 수백만개의 채널을 관리하는 것도 가능하다._

이때 pub/sub이 가벼운 이유는, 최소한의 메시지 전달 기능만 제공하기 때문이다.

- **발행자**: 메시지를 채널에 보낼 수만 있을 뿐이고, 어떤 구독자가 메시지를 읽어가는지 혹은 메시지가 정상적으로 전달되었는지는 확인할 수 없다.
- **구독자**: 메시지를 받을 수만 있을 뿐이고, 해당 메시지가 어떤 발행자에 의해 언제 생성되었는지 등의 메타데이터는 알 수 없다.
- 뿐만아니라 한 번 전파된 메시지는 레디스에 저장되지 않는다.

즉 pub/sub은 단순히 메시지의 통로 역할만 수행하기 때문에, 정합성이 요구되는 경우에는 적합하지 않은 기술일 수 있다. 만약 이와 같은 서비스에서 pub/sub을 사용해야 한다면 애플리케이션 레벨에서 관련 로직을 추가해주어야 한다.

## 클러스터 구조에서의 Pub/Sub

레디스 클러스터 구조에서도 pub/sub을 사용할 수 있다.

하지만 이 경우, 각 클러스터 노드는 해시 슬롯, 즉 key를 기준으로 분산되기 때문에, pub/sub의 경우 채널은 분산되지 않는다. 즉 모든 노드가 전체 채널을 다 같이 관리하게 된다. 클러스터 내의 아무 노드에 SUBSCRIBE 커맨드를 사용하면 정상적으로 메시지를 전달받을 수 있는 것이다.

따라서 발행자가 메시지를 발행하면 클러스터 내의 모든 노드에 해당 메시지가 전부 전달된다.

![redis-6-6.jpeg](attachment:ed8c0e75-40d7-460f-846d-471293f69f7d:redis-6-6.jpeg)

해당 방식은 간단하기는 하지만, 채널 기반으로 분산되지 않기 때문에 부하 분산의 효과는 볼 수 없다고 한다. 이로 인해 불필요한 리소스가 사용되고 부하가 증가할 수 있다는 문제를 가지고 있다.

## Shared Pub/Sub

shared pub/sub은 클러스터 구조에서의 pub/sub의 문제를 해결하기 위해, redis 7.0 버전에 도입된 기능이다.

shared pub/sub 환경에서는, 각 채널이 해시 슬롯에 할당된다. 따라서 각 노드에 채널이 분산되는 것이다. 메시지 발행에 대해서도 발행된 메시지가 모든 노드에 전달되지 않고, 같은 슬롯을 가지고 있는 노드 간에만 전달된다. _(= 복제 노드에만 전달된다.)_

![redis-6-7.jpeg](attachment:7f0ff3cf-7151-4d93-af15-f761c0181fbb:redis-6-7.jpeg)

shared pub/sub 기능을 통해 클러스터 구조에서 비교적 효율적으로 메시지를 전파할 수 있게 되었다.

# 1. 문제 이해 및 설계 범위 확정

다음 질의응답을 통해 구체적인 요구사항을 알아본다.

```
1. '주변'으로 판단되는 지리적 거리는 얼마인지?
>> 기본적으로 5마일(약 8km)이며, 해당 수치는 설정 가능함.

2. 해당 거리는 직선 거리인지, 아니면 실제 해당 사용자의 위치로 이동할 수 있는 이동 거리인지?
>> 직선거리로 가정

3. 얼마나 많은 사용자가 이 기능을 사용하는지?
>> 앱 사용자는 10억명 정도이며, 그 중 10%가 해당 기능을 사용함

4. 사용자의 이동 이력을 보관해야 하는지?
>> 머신러닝 등 다양한 용도로 사용될 수 있으므로 보관할 것

5. 친구 사용자가 10분 이상 비활성 상태를 유지할 경우 주변 친구 목록에서 사라지게 할지, 아니면 마지막 위치를 표시하게 할지?
>> 목록에서 사라지게 할 것

6. GDPR이나 CCPA와 같은 사생활 및 데이터 보호법 고려해야 할지?
>> 설계가 너무 복잡해질 수 있으므로 배제
```

위 질의응답을 바탕으로 요구사항을 정리해보면 다음과 같다.

> 기능 요구사항

- 사용자는 앱 사용 시 주변 친구를 확인할 수 있다.
  → 주변 친구 목록에서 해당 친구까지의 거리, 마지막으로 갱신된 시각(timestamp)가 표시된다.
- 친구 목록은 수 초마다 갱신되어야 한다.
- 주변 친구 목록은 페이지마다 20명의 친구를 표시한다.

> 비기능 요구사항

- **지연 시간**: 주변 친구의 위치 변화가 실시간으로 반영될 필요는 없지만, 너무 오랜 시간이 걸려서는 안된다.
- **안정성**: 전반적으로 안정적인 시스템이어야 하지만, 때로 일부 데이터가 유실되는 정도는 용인된다.
- **결과적 일관성**: 위치 데이터를 저장하기 위해서 강한 일관성을 지원하는 DB를 사용할 필요는 없다. 또한, 복제의 데이터가 원본과 동일하게 변경되기까지 몇 초 정도 지연되는 것은 용인된다.

> 개략적 규모 추정

- **주변 친구 기준**: 사용자 반경 5마일 이내
- **주변 친구 위치 갱신 주기**: 30초
  → 사람이 걷는 속도는 빠르지 않기 때문에, 30초 간격의 갱신으로도 충분히 문제 없이 구현 가능하다.
  _교통수단으로 빠르게 이동하는 경우?_
- **DAU**: 1억 명
- **동시 접속 사용자**: DAU의 10%로 가정 → 1,000만 명
- **사용자 평균 친구 수**: 400명

⇒ **위치 정보 갱신 QPS**: (동시 접속 사용자 1,000만명) / (갱신 주기 30초) = 약 334,000

# 2. 개략적 설계

## 설계 아이디어

해당 설계의 목적은, 사용자 인근의 활성 상태인 친구 사용자들로부터 위치 정보를 주기적으로 수신하도록 하는 것이다. 때문에 단순히 이론적으로 생각해보면, 인근 친구 사용자들과 직접 연결을 수립하는 P2P 방식을 떠올려볼 수 있다.

![system-2-2.jpeg](attachment:30ab683f-b593-4107-8dda-cb1ff8192b6a:system-2-2.jpeg)

현실적으로 모바일 기기는 통신 상태가 안정적이라는 보장이 없고, 전력량도 충분치 않기 때문에 효과적인 방법이라고 보기는 어렵다. 다만 해당 아이디어를 바탕으로 설계 방향을 정해볼 수 있다.

다음의 설계와 같이, 백엔드를 사이에 두고 주변 친구 사용자들과 연결을 맺는 방법이다.

![system-2-3.jpeg](attachment:adea8674-0263-48c7-870d-e5c6cefd554d:system-2-3.jpeg)

위 설계에서 백엔드는 다음의 역할을 수행한다.

1. 모든 활성 상태의 사용자들의 위치 변경 내역을 수신한다.
2. 사용자의 위치 변경 내역을 수신할 때마다, 해당 사용자의 친구 사용자 중 활성 상태이면서 인근에 위치한 사용자들에게 변경 내역을 전달한다.

이와 같은 아이디어를 바탕으로 해당 시스템을 설계해볼 수 있다.

## 개략적 설계안

우선 QPS 규모를 고려하지 않고, 위 아이디어를 바탕으로 개략적 설계를 수행해본다.

_이 설계만으로는 요구된 규모를 감당할 수 없다. 규모 추정에 따라 해당 시스템은 초당 334,000건의 위치 변경을 처리해야 한다. 각 사용자 친구 수 평균인 400명 중에서 활성 상태인 주변 친구 수가 10%라고 가정해도, 초당 334,000건 X 40명 = 약 1,400만 건의 거리 계산을 수행하고 이 데이터를 각 단말로 전송해야 한다._

![system-2-4.jpeg](attachment:434953af-b5aa-47f7-9a5e-34d46cd6ea2a:system-2-4.jpeg)

### 컴포넌트 설명

개략적 설계안 상의 각 컴포넌트에 대한 설명이다.

> 로드밸런서

트래픽을 각 서버로 분배한다.

> API 서버

해당 모바일 앱의 일반적인 기능을 수행하는 API 서버이다. 사용자의 요청에 따라 사용자 DB와 통신하면서 작업을 수행한다.

> 웹소켓 서버

주변 친구 기능을 담당하는 유상태 서버이며, 각 클라이언트는 위치 정보 갱신을 위해 해당 서버와 웹소켓 연결을 유지한다.

- redis pub/sub 서버와 통신하며, 주변 친구 사용자에 대한 위치 변경 사항을 처리한다.
- 비활성 상태의 사용자가 활성 상태로 연결될 경우, 모든 주변 친구를 응답하는 초기화 작업도 수행한다.

_현대 프로그래밍 언어들은 웹소켓 연결 유지에 많은 자원을 필요로 하지 않기 때문에, 지속적 연결을 유지하는 방식으로 설계하였다._

> Redis Pub/Sub 서버

활성 상태의 모든 사용자에 대한 채널을 관리하며, 웹소켓 서버와 통신하면서 위치 변경에 대한 메시지를 발행하는 역할을 수행한다.

![system-2-6.jpeg](attachment:8657ed82-7ac3-42c7-9136-cb9e4d688c94:system-2-6.jpeg)

<aside>
💡

**Redis Pub/Sub을 선택한 이유?**

가장 먼저 redis pub/sub으로 채널을 생성하는 것은 매우 적은 비용이 들며, GB 단위의 메모리를 가진 redis 서버라면 수백만개의 채널을 관리하는 것도 가능하다.

이는 redis pub/sub이 신뢰성을 포기한 경량 메시지 브로커이기 때문인데, 해당 시스템의 경우 요구사항에 따르면 위치 변경이 가끔 누락되더라도 크게 문제가 없기 때문에 적절한 기술로 선택되었다.

</aside>

> 사용자 DB

일반적인 사용자 관련 데이터, 친구 관계 데이터 등을 관리하는 DB이다. 대규모 사용자를 관리하기 위해 RDBMS를 선택해 샤딩하거나, 확장성이 좋은 NoSQL을 선택할 수도 있다.

> 사용자 위치 이동 내역 DB

사용자의 위치 변경 내역을 관리하는 DB이다.

_해당 기능 자체에 크게 관련된 DB는 아니다. 단순 요구사항을 위해 설계에 포함되었다._

> 사용자 위치 정보 Cache

활성 상태 사용자의 가장 최근 위치 정보를 캐싱한다. 이때 TTL 값을 활성 상태 기준 시간으로 설정하여, TTL로 인해 해당 사용자의 키가 만료되면 비활성 상태로 판단한다. 해당 예제에서는 Redis를 채택하였다.

### 주기적 위치 갱신 시나리오

위 설계를 바탕으로, 사용자의 위치가 갱신되는 흐름을 정리해보면 다음과 같다.

![system-2-7.jpeg](attachment:efa24238-ab9b-4a77-8d50-c93d39077802:system-2-7.jpeg)

1. 각 클라이언트 앱이 실행되면서 웹소켓 서버와 연결을 맺는다.
2. 클라이언트 앱은 주기적으로 위치 변경 사항을 로드밸런서로 전송한다.
3. 로드밸런서는 해당 데이터를 웹소켓 서버로 전달한다.
4. 웹소켓 서버는 위치 변경 사항을 ‘사용자 위치 이동 내역 DB’에 저장한다.
5. 웹소켓 서버는 위치 변경 사항을 ‘사용자 위치 정보 Cache’에 저장한다. _이때 TTL 역시 갱신된다._
6. 웹소켓 서버는 위치 변경 사항을 ‘Redis Pub/Sub 서버의 사용자 채널’에 발행한다.
7. Redis Pub/Sub 채널에 발행된 위치 변경 사항이 구독자들에게 전파된다.
   _이때 구독자는 해당 사용자의 친구 중에서 현재 웹소켓 서버에 연결되어 있는 활성 상태 사용자들이다._
8. 위치 변경 사항을 전파받은 웹소켓 서버는, 메시지를 발행한 사용자와 구독한 사용자 간의 거리를 계산한다.
9. 만약 현재 거리가 반경 이내라면 구독 사용자의 클라이언트 앱에 해당 데이터를 전송한다.
   만약 현재 거리가 반경을 넘는다면 전송하지 않는다.

위의 과정을 사용자와 사용자 간의 관점에서 다시 나타내보면 다음과 같다.

![system-2-8.jpeg](attachment:bcb1eb5a-42b8-416b-a12a-8143a11602f6:system-2-8.jpeg)

1. 사용자 1의 위치가 변경되면, 위치 변경 사항이 웹소켓 연결을 통해 웹소켓 서버에 전송된다.
2. 해당 위치 변경 사항은 Redis Pub/Sub 서버의 해당 사용자 채널로 발행된다.
3. 해당 사용자 채널의 모든 구독자에게 위치 변경 사항이 전파된다.
4. 위치 변경 사항을 전파받은 웹소켓 연결 핸들러는 발행자와 구독자 사이의 거리를 계산해서, 반경 이내에 존재할 경우 이 변경 사항을 구독자 클라이언트 앱에 전송한다.

### 클라이언트 초기화 시나리오

‘주변 친구’ 기능 사용자가 클라이언트 앱을 실행하게 되면, 현재 주변 친구 목록을 초기화해주는 작업이 필요하다. 이러한 클라이언트 초기화 작업의 흐름을 정리해본다.

클라이언트 앱이 실행되고 웹소켓 연결이 초기화되면, 클라이언트는 해당 사용자의 위치 정보를 웹소켓 서버에 전송한다. 해당 정보를 받은 웹소켓 핸들러는 다음 작업을 수행하게 된다.

![스크린샷 2025-04-07 오후 8.19.30.png](attachment:0ad702b1-9c75-4305-8ef0-a834a849475d:스크린샷_2025-04-07_오후_8.19.30.png)

1. 위치 정보 캐시에 해당 사용자의 위치를 저장한다.
2. 사용자 정보 DB로부터, 해당 사용자의 모든 친구 데이터를 읽어온다.
3. 위치 정보 캐시에, 친구 데이터를 바탕으로 모든 친구들의 현재 위치 정보를 일괄 조회한다.
4. 조회한 친구들의 위치 정보와 현재 사용자의 위치 정보를 가지고 거리를 계산한다.
5. 반경 내의 친구에 대해 해당 친구의 정보와 위치, 타임스탬프를 클라이언트 앱에 전송한다.
6. 이후 웹소켓 서버는 활성 여부에 관계 없이 해당 사용자의 모든 친구들의 redis pub/sub 채널을 구독한다.
   _채널을 생성하고 구독하는 비용이 저렴하며, 비활성화 상태의 경우 전혀 작업이 일어나지 않기 때문에, 사용하는 비용에 비해 설계가 단순해진다는 장점이 더 크다._
7. 마지막으로 해당 사용자의 현재 위치 정보를 redis pub/sub 채널에 발행한다.

<aside>
❓ 웹소켓 대신 SSE를 사용하면 안되나요?
</aside>

## API 설계

웹소켓 서버에서 제공되어야 하는 API를 간략하게 설계한다. 이때 API 서버의 HTTP API는 생략하고, 웹소켓 서버의 웹소켓 API에 대해서만 설계한다.

### 서버 API

**위치 정보 갱신 API**: 위도/경도 및 시간 정보를 요청받아 위치를 갱신한다. 별도의 응답은 없다.

**클라이언트 초기화 API**: 위도/경도 및 시간 정보를 요청받아, 클라이언트의 친구들 위치 정보를 응답한다.

### 클라이언트 API

**친구 위치 정보 갱신 API**: 친구의 위치 정보 및 변경 시간 타임스탬프를 전송받는다.

**새 친구 구독 API**

**친구 구독 해지 API**

## 데이터 모델 설계

해당 설계안에서 사용되는 각 DB의 데이터 모델을 조금 더 자세히 설계해본다.

### 사용자 위치 정보 캐시

위치 정보 캐시는, ‘주변 친구’ 기능을 사용하는 사용자들 중 활성 상태인 사용자의 가장 최근 위치를 저장한다. 해당 캐시로는 Redis를 선택했으며, 다음과 같은 key-value 쌍으로 위치 정보를 보관한다.

| key       | value              |
| --------- | ------------------ |
| 사용자 ID | {위도, 경도, 시각} |

_이때 레디스의 별도 자료구조를 사용하지 않고 단순히 key를 사용하고 있는데, 레디스의 key는 내부적으로 해시 테이블로 관리되기 때문에 상수 시간에 접근할 수 있다. 별도의 자료구조를 사용하지 않아 메모리 역시 효율적으로 사용 가능하다._

<aside>
💡
**DB가 아닌 캐시에 위치 정보를 보관하는 이유?**

‘주변 친구’ 기능에서는 각 활성 사용자의 현재 위치만을 필요로 한다. 즉, 해당 데이터에 대해서는 영속성이 보장될 필요가 없다는 것이다. 이와 같은 이유로 영속성을 비교적 보장하지 않지만 연산이 빠른 캐시를 선택한 것이다.

만약 캐시 서버에 장애가 발생한다 하더라도, 잠시 해당 기능에 문제가 생기겠지만 위치 갱신 주기가 빠르기 때문에 빠른 시간 내에 캐시가 warmed-up될 수 있다.

특히 Redis는 각 키에 대해 TTL을 지원하기 때문에, TTL 만료 여부를 통해 사용자의 활성 상태를 별도의 작업 없이 관리할 수 있다는 장점도 있다.

</aside>

### 사용자 위치 이동 내역 DB

위치 이동 내역 DB에는 다음과 같은 스키마로 사용자의 위치 이동 내역을 저장한다.

| user_id | latitude | longitude | timestamp |
| ------- | -------- | --------- | --------- |

이때 해당 DB는 굉장히 많은 쓰기 연산을 수행해야 하기 때문에 이에 따르는 부하를 감당할 수 있고, scale-out이 용이한 DB를 선택해야 한다.

RDBMS를 사용하는 경우 샤딩이 필요하게 되며, 카산드라와 같은 NoSQL을 선택하는 것도 적절할 수 있다.

# 3. 상세 설계

![system-2-4.jpeg](attachment:90249d26-6254-4bcc-8bb2-1c8fe84bac83:system-2-4.jpeg)

위와 같이 개략적 설계를 통해 대부분의 요구사항을 만족하는 시스템을 설계했지만, 주어진 규모의 트래픽을 감당할 수는 없다. 각 컴포넌트 별로 병목을 찾아 해결하는 상세 설계를 진행한다.

## API 서버 확장

대규모 트래픽을 처리할 수 있도록 API 서버를 scale-out하는 것을 고려해야 한다. 이때 무상태 API 서버를 클러스터링하는 방법은 다양하고 잘 알려져 있기 때문에 자세히 다루지 않는다.

_CPU 사용률, 부하, IO 상태 등의 기준으로 오토스케일링하는 방법이 대표적이다._

## 웹소켓 서버 확장

웹소켓 서버 역시 클라이언트들과 연결을 유지해야 하기 때문에, 대규모 트래픽 처리를 위한 scale-out이 필수적이다. 해당 서버의 오토스케일링 역시 크게 어렵지는 않지만, API 서버와 달리 stateful 서버이기 때문에 이에 대한 추가적인 고려는 필요하다.

먼저 서버 클러스터에서 서버를 제거해야 하는 경우, 각 서버는 연결을 유지하고 있기 때문에 바로 서버를 제거해서는 안 된다. 따라서 삭제하고자 하는 서버를 로드밸런서가 ‘종료 중(draining)’ 상태로 인식하게 하여, 새로운 연결이 수립되지 않도록 설정한다. 이후 기존 연결이 모두 종료되면 그 시점에 서버를 제거해야 한다.

또한 새로운 버전의 애플리케이션을 배포하는 경우에도 주의해야 한다.
_blue-green 배포를 통해 기존 버전의 서버와 새 버전의 서버를 함께 유지하면서, 트래픽이 기존 서버를 향하지 않도록 draining 상태로 변경한다. 이후 기존 서버의 연결이 모두 해제되면 기존 서버를 제거하는 방식을 사용할 수 있을 것이다._

즉 로드밸런서의 역할이 중요하며, 대부분의 클라우드 로드밸런서는 이와 같은 기능을 제공한다.

## 사용자 정보 DB 확장

각 사용자의 정보와 친구 관계를 관리하는 사용자 정보 DB에는 약 10억 명의 사용자 정보가 저장된다.

RDBMS의 경우 해당 규모의 데이터를 한 대의 서버로 관리하는 것이 불가능하기 때문에, 사용자 id를 기준으로 샤딩해야 한다.

_이때, 해당 규모와 같은 시스템에서는 사용자 정보를 관리하는 별도의 팀이 존재할 것이며, 이 경우 DB에 직접 조회를 수행하는 것이 아닌 별도의 사용자 관련 서비스의 API 요청을 통해 데이터를 조회하게 될 것이다._

## 사용자 위치 정보 캐시 확장

위치 정보 캐시의 경우, 메모리 관점에서는 고성능의 redis 서버 한 대로도 충분히 대용량의 데이터를 관리할 수 있다. 각 key에는 TTL이 설정되어 있어 사실상 특정 시점의 접속자의 위치 정보만 저장하기 때문에, 최대 메모리 사용량이 일정 한도 이내로 유지된다.

예를 들어 최대 동시 접속자 수가 1,000만 명이고 각 key는 100바이트의 메모리를 사용한다고 가정하면, 몇 GB의 메모리로도 충분히 이를 감당할 수 있다.

하지만 CPU 관점에서는, 초당 334,000 건에 달하는 위치 정보 갱신 연산을 수행해야 한다. 이는 redis 서버 한 대로는 감당하기 어려운 트래픽이다.

따라서 redis 서버의 샤딩을 고려해야 하며, 추가적으로 각 샤드에 해당하는 노드마다 standby 노드를 두어 가용성을 높일 수 있다.

## Redis Pub/Sub 서버 확장

‘주변 친구’ 기능의 사용자를 1억 명으로 가정했기 때문에, redis pub/sub 서버에는 1억개의 채널이 관리되어야 한다.

메모리 관점에서, 하나의 채널을 관리하는 데 20바이트가 필요하다고 가정하면 전체 채널 관리를 위해 약 200GB를 필요로 하게 된다. 높은 메모리 사양의 서버를 사용한다면 서버 몇 대로 충분히 1억개의 채널을 관리할 수 있다.

CPU 관점에서는, 초당 약 1,400만 건의 위치 정보 변경 사항을 구독자들에게 전송해야 한다. 해당 트래픽을 감당하기 위해서는 서버 몇 대로는 충분하지 않을 수 있다.

예를 들어 서버 한 대로 초당 처리할 수 있는 작업의 수가 10만 건이라고 가정해보자. 이 경우 서버는 총 140대가 필요하게 된다. _이는 매우 보수적인 수치로, 이보다 훨씬 많은 작업이 가능할 것이다._

결론적으로 해당 규모에서는 Redis Pub/Sub 서버는 분산 운영되어야 한다.

### 1. 서비스 디스커버리 컴포넌트 도입

해당 설계에서는 Redis Pub/Sub 클러스터를 구성하기 위해 서비스 디스커버리 컴포넌트를 사용한다. etcd나 Zookeeper와 같은 스택을 사용할 수 있으며, 아주 기본적인 기능만을 활용한다.

1. 가용 서버 목록을 유지하고 갱신하는 기능: key-value 저장소로 활용
2. 가용 서버 목록의 변경 내역을 클라이언트가 구독하는 기능

디스커버리 컴포넌트의 key-value 저장소에는, 가용한 Redis Pub/Sub 서버로 구성된 안정 해시 링을 저장한다.

![system-2-9.jpeg](attachment:59effb1b-7e6f-4528-84fd-c0df0d7f181f:system-2-9.jpeg)

이를 통해 특정 채널에 메시지를 발행하거나 구독해야 하는 경우, 해당 채널이 어떤 pub/sub 서버에 존재하는지 조회할 수 있다.

이 구조에서, 특정 채널에 위치 변경 사항을 발행하는 과정은 다음과 같다.

![system-2-10.jpeg](attachment:ef53ee83-4262-4ec0-afc5-52a22cf8b3d6:system-2-10.jpeg)

1. 웹소켓 서버는 디스커버리 컴포넌트의 해시 링을 참조해서, 해당 사용자 채널이 어떤 서버에 있는지 확인한다.
   _이때 성능을 고려하면, 해당 해시 링을 웹소켓 서버에 캐싱해두는 방법을 사용할 수 있다. 단, 이 경우 원본에 변경이 발생하는 경우 이를 바로 반영할 수 있도록 디스커버리 서비스에 구독 관계를 설정해두어야 한다._
2. 해당 서버에 위치하는 사용자 채널에 위치 변경 사항을 발행한다.

_구독할 채널을 찾는 과정도 이와 동일하다._

### 2. 규모 변경 고려사항

이와 같이 구축한 Redis Pub/Sub 클러스터의 규모를 어떻게 확장시킬지에 대해 알아본다.

확장 방식을 결정하기 위해, Redis Pub/Sub 서버의 특징을 되짚어보면 다음과 같다.

- **무상태 서버?**
  : pub/sub의 메시지는 redis에 보관되지 않고, 발행이 끝나면 바로 사라진다. 이런 관점에서, redis pub/sub 서버는 무상태 서버라고 볼 수 있다.
- **유상태 서버?**
  : pub/sub 서버는 채널에 대한 정보를 보관한다. 어떤 클라이언트가 특정 채널을 구독하고 있는지를 관리해야 하기 때문이다. 따라서 클러스터에 변경이 생기면 채널 역시 함께 다른 서버로 옮겨지게 되고, 해당 채널을 구독하고 있던 클라이언트들은 새로 옮겨진 서버에 재구독을 요청해야 한다. 이런 관점에서, pub/sub 서버는 유상태 서버라고 볼 수 있다.

결론적으로 pub/sub은 유상태 서버로 취급하는 것이 좋다. 유상태 서버의 규모를 변경하는 것은 위험이 큰 작업이기 때문에 신중히 고려해야 하며, 보통은 크기 변화를 막기 위해 여유 있는 규모로 over provisioning하는 경우가 많다.

_만약 해당 구조에서 클러스터 구성이 변경되는 경우, 수많은 채널들이 새로운 서버로 옮겨지게 되면서 이에 대한 수많은 재구독 요청이 발생하게 될 것이다. 재구독 요청이 쏟아지면, 이를 처리하느라 서비스 기능 요청 처리가 늦어지거나 누락될 수 있다는 문제가 발생한다._

해당 클러스터의 크기를 조정하는 방법은 다음과 같다.

1. 해시 링을 갱신한다. 만약 해시 링의 크기가 늘어나는 경우 새 서버를 추가한다. (?)
2. 디스커버리 서비스에 저장된 기존 해시 링을 새 해시 링으로 갱신한다.

### 3. 서버 교체 고려사항

이번에는 클러스터 내의 노드 하나를 교체하는 상황에 대해 고려해본다. 서비스를 운영하다 보면 서버 장애는 종종 발생하게 되는데, 이 경우에는 클러스터 규모가 변경되는 상황에 비해 비교적 위험이 적다.

_규모 변경의 경우 모든 서버들의 채널이 옮겨지게 되지만, 서버 교체의 경우에는 교체 대상 서버의 채널들만 옮겨지기 때문이다._

서버 장애로 인한 교체 시나리오는 다음과 같다.

![system-2-11.jpeg](attachment:24542842-55f7-4876-a0b2-0cde34179da2:system-2-11.jpeg)

1. pub/sub 클러스터 노드 중 하나에 장애가 발생한다.
2. 모니터링 서비스는 장애를 감지하고, 관리자에게 경보를 보낸다.
3. 관리자는 디스커버리 서비스의 해시 링에서, 장애 서버를 새로운 서버로 변경하여 다시 저장한다.
4. 해시 링의 변경 내역이 디스커버리 서비스를 구독하고 있던 웹소켓 서버에 전송된다.
5. 웹소켓 서버의 각 웹소켓 핸들러들은 구독 중인 채널들 중 이동된 채널들에 대해 재구독을 요청한다.

<aside>
❓
Discovery Service VS Shared Pub/Sub

Redis 자체적으로 제공하는 Shared Pub/Sub을 사용하면 직접 안정 해시를 구현하고 서비스 디스커버리 컴포넌트를 추가할 필요가 없지 않을까요?

</aside>

## 친구 추가/삭제

서비스 사용 도중 친구를 추가/삭제하는 경우, 해당 사용자의 웹소켓 핸들러는 새 채널을 구독하거나 기존 구독 채널을 취소해야 한다.

이는 콜백을 통해 구현할 수 있다.

**친구 추가**: 친구를 추가한 경우, 콜백을 통해 웹소켓 핸들러가 해당 친구의 채널을 구독하도록 메시지를 보낸다.

**친구 삭제**: 친구를 삭제한 경우, 콜백을 통해 웹소켓 핸들러가 해당 친구의 채널 구독을 취소하도록 메시지를 보낸다.

_해당 설계는 친구가 위치 정보 사용을 허가/취소하는 경우에도 활용할 수 있다._

## 친구가 많은 사용자

SNS 서비스에서는 항상 고려되어야 하는 문제이다. 한 사용자가 평균적인 사용자들에 비해 훨씬 많은 친구 목록을 가지는 경우, 해당 사용자로 인해 발생할 수 있는 부하를 고려해야 한다.

해당 예제에서는 최대로 맺을 수 있는 친구의 수가 정해져 있다고 가정한다. _페이스북의 경우 5,000명이라고 한다_

> 웹소켓 서버

해당 사용자가 수천 명의 친구를 구독하는데 필요한 pub/sub 구독 관계는 클러스터 내의 많은 웹소켓 서버에 분산되어 있다. 따라서 수많은 친구들의 위치가 변경되는 것에서 오는 부하는 각 웹소켓 서버가 나누어 처리하기 때문에, 웹소켓 서버에서는 hotspot 문제를 걱정하지 않아도 된다.

<aside>
❓hotspot 문제

- 헤비 유저가 자신의 위치 변경을 발행하는 경우에는, 각 친구들의 웹소켓 연결이 여러 서버에 분산되어 있어서 각 서버가 부하를 분산하게 됨.
- 반대로 헤비 유저가 수천 명의 친구들의 위치 변경을 구독하는건 하나의 웹소켓 연결 핸들러에서 수행되는거 아닌가요? 그럼 한 서버에 위치 변경 처리 작업이 몰리면서 부하가 집중될 것 같은디 왜 걱정하지 말라는지 모르겠음
</aside>

> pub/sub 서버

pub/sub 클러스터 내의, 해당 사용자의 채널이 존재하는 노드의 경우에는 많은 부하를 감당하게 된다. 하지만 현재 설계에서 pub/sub 서버는 100대 이상의 노드로 구성되어 있기 때문에 수천 명 정도의 구독자를 가진 채널을 감당할 수 있으며, 이와 같은 헤비 유저의 채널들이 여러 노드에 분산된다는 점에서, 막대한 피해를 주지는 않는다고 판단할 수 있다.

_확률은 낮겠지만 해당 분산 구조에서는 헤비 유저들이 한 노드에 몰리는 경우도 발생할 수는 있을 듯함. 이런 경우에 대비해서 각 노드의 자원 사용률을 모니터링하고 실제로 한 노드에 몰리는 경우 적절히 직접 분산시켜주어야 할듯_

## 주변의 임의 사용자

해당 기능에, 주변의 모든 사용자에 대한 위치 정보를 보여줘야 하는 요구사항이 추가된다면 어떻게 설계할 수 있을까?

해당 설계를 벗어나지 않으면서 추가 기능을 구현하는 방법으로, 이전에 알아보았던 지오해시를 활용하는 방법이 있다. 다음과 같이 지오해시의 각 격자마다 pub/sub 채널을 하나씩 할당하는 것이다.

![system-2-12.jpeg](attachment:d8cc8663-b183-4122-8f63-13188e9669bc:system-2-12.jpeg)

다음과 같이, 해당 격자 내에 위치하는 사용자들은 해당 격자의 채널을 구독하고 위치 변경 사항을 발행하게 된다.

![system-2-13.jpeg](attachment:c140e73b-9e1c-40b8-a242-6a2a94c3229d:system-2-13.jpeg)

위치 변경 시나리오는 다음과 같다.

1. 사용자의 위치가 변경되면, 웹소켓 서버는 사용자의 위치를 바탕으로 지오해시 ID를 계산한다. 이후 해당 ID에 해당하는 채널에 위치 변경 내역을 발행한다.
2. 해당 채널을 구독하고 있던 인근 사용자들에게 해당 위치 변경 내역이 전파된다.

이때 지오해시에서의 edge case를 고려하면, 다음과 같이 사용자가 위치한 격자뿐만아니라 인근의 격자에 대한 채널까지 구독함으로써 경계 문제를 처리할 수 있다.

![system-2-14.jpeg](attachment:5916abbb-b6d3-4285-a0b9-8ad1d1557a66:system-2-14.jpeg)

## Redis Pub/Sub의 대안

Erlang은 고도로 분산된 병렬 애플리케이션을 위한 언어이자 런타임 환경이다. 이를 활용하면 해당 설계를 오히려 더 효율적으로 개선할 수도 있다. 얼랭ㅋㅋ

얼랭은 경량 프로세스라는 큰 장점을 가지며, 최신식 서버를 사용할 경우 서버 한 대에서 수백만 개의 프로세스를 실행할 수도 있다고 한다. 뿐만아니라, 아무 작업을 하지 않는 얼랭 프로세스는 CPU를 전혀 사용하지 않는다고 한다.

즉, 해당 설계의 활성 사용자들을 각각 얼랭 프로세스로 모델링할 수 있다는 것이다. 활성 사용자 한 명 당 프로세스 하나를 할당해버려도 비용이 크지 않다.

**사용자**: 경량 얼랭 프로세스로 모델링한다. 사용자 한 명 당 프로세스 하나를 할당해버리는 것이다.

**웹소켓 서버**: 얼랭으로 구현한다.

**메시지 브로커**: 분산 얼랭 애플리케이션으로 대체한다. 친구 관계의 사용자들의 프로세스 간 구독하게 하면 메시지 브로커가 필요 없다.

_사실 얼랭이라는 기술이 그렇게 널리 사용되고 있지 않기도 하고, 실제로 이 기술을 배워서 적용해라 라는 의미보다는 해당 요구사항에 맞춰 이런 식으로도 최적화할 수 있다는 점을 보여주는 예시가 아닐까 싶음._
